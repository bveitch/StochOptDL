\documentclass[nohyperref]{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{hyperref}

\usepackage[accepted]{icml2022}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
%\usepackage[capitalize,noabbrev]{cleveref}
%\usepackage{lscape}

\usepackage{enumitem}
%%%%%%%%%%%%%%
%\usepackage[utf8]{inputenc}
%\usepackage[english]{babel}
\usepackage{natbib}
\bibliographystyle{plainnat}
%\addbibresource{Milestone2_0}
%%%%%%%%%%%%%%%%%%
%default package
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\DeclareMathOperator*{\Diag}{Diag} 
\DeclareMathOperator*{\sgn}{sign}
%\DeclareMathOperator{\spn}{span} 
%\DeclareMathOperator*{\sn}{sn}
%\DeclareMathOperator*{\cn}{cn} 
%\DeclareMathOperator*{\Real}{Re}
%\DeclareMathOperator*{\Imag}{Im} 
%\DeclareMathOperator*{\sinc}{sinc}
%\DeclareMathOperator*{\Tr}{Tr}\DeclareMathOperator*{\grad}{grad}
%\DeclareMathOperator*{\Lim}{lim}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
%%\input{macros.tex}
%\numberwithin{equation}{section}

\begin{document}

\twocolumn[
\icmltitle{Second-order optimisation methods in Deep Learning with applications on the Fashion MNIST dataset}
\icmlsetsymbol{equal}{*}
\begin{icmlauthorlist}
\icmlauthor{Ben Veitch}{equal,comp}
\end{icmlauthorlist}

\icmlaffiliation{comp}{}

\icmlcorrespondingauthor{Ben Veitch}{}
\icmlkeywords{Machine Learning, Numerical Optimization, Deep learning}

\vskip 0.3in
]

\date{\today}

\begin{abstract}
I present an empirical study into the use of second-order solvers, L-BFGS and NLCG for training neural networks. Comparisons are made on the Fashion MNIST dataset and I show improved performance in terms of convergence and dev-accuracy over Gradient descent. Whilst L-BFGS and NLCG have a higher cost per iteration than gradient descent, being roughly twice that of gradient descent, the improvement in total training time might more than compensate for the increased iteration cost. I show that the use of stochastic sampling which is required for efficient training on a large dataset does harm the convergence of these solvers. As such, batches must be fixed throughout the training epochs. I also present a sufficient condition, Theorem \ref{thm:bigtheorem}, for convexity of the loss-function during training. I argue that this can incorporated into a Gauss-Newton scheme to determine effective preconditioning strategies for improving convergence during training. 
\end{abstract}

\section{Introduction}
Large-scale optimisation methods are fundamental to machine learning. This is due to the ubiquity of large datasets in the modern world and the nature of the numerical methods which are used to train a neural network.  For example, the Fashion-MNIST dataset \citep{FashionMNIST:data} consists of 60,000 training images of 10 clothing types with 28 $\times$ 28 pixels in each image, this is around 470M parameters. To train an accurate model to adequate performance on a large dataset can require neural networks with many layers; LeNet-5 \citep{LeCun:5} had 7 layers, whereas later versions of Resnet \citep{ResNet_18} have 158, and numbers in excess of 1200 are reported as being possible \citep{Huang:Stoch}. This leads to many millions of parameters for the neural network weights all of which must be optimized for. The development of algorithms which are scalable with respect to data size whilst also being fast to converge is therefore key to the success of deep learning methods.

The ability to quickly train sets of neural network weights has the advantage of reducing the cost of cross-validation of hyper parameters which leads to an improvement in the test accuracy (low bias, low variance) of deep neural networks. When neural networks are slow to converge this leads to long turnaround time and additional confusion can be introduced by extracting the weights before the solver has finished converging. A high-variance solution may be avoided at the expense of higher bias, whereas in this case neither of these are legitimately associated with the hyper-parameters of the network itself. 

However, the challenge associated with training very deep networks is not simply one of efficiency. Very often a first-order method is used, (using only the local gradient) and this leads to significant theoretical problems. A well publicised example is the problem of `vanishing-exploding gradients' \citep[see][]{Huang:Stoch}. Here, the weights associated to a particular layer can become vanishingly small or exponentially large, both of these greatly reduce the effective size of the weights in that layer whilst also introducing numerical errors. This leads to a loss of predictive power for the network. A second problem is the existence of saddle points on the surface of the loss function \citep[see][]{Martens:Deep}. These pose a difficulty for any solver but are particularly pathological if only the local gradient is considered. In this case, the loss function will likely become trapped in a local minimum and will also lose predictive power. A final problem is the determination of an accurate step-length. Clearly, when the step-length is poorly estimated, even for a convex function with an accurate local gradient the resulting weights update can be a long way from the true minimum and convergence will be slow.  
   
In contrast to gradient descent, second-order numerical optimization methods such as Newton's method, non-linear conjugate gradients (NLCG) or BFGS can converge quickly \citep[see][]{Nocedal:Wright}. They owe much of this fast convergence to capturing some of the local curvature of the optimization surface and therefore avoid problems associated to steep valleys, corners and saddle points which are typically present in the minimum surface of a nonlinear objective function. However, the ability to capture local curvature requires an approximate Hessian and this can become computationally expensive for models with millions of parameters. The advantage of NLCG and BFGS is that they can efficiently approximate the action of the Hessian without incurring the full cost of a Newton scheme.

On the other hand stochastic descent methods which draw random samples, or mini-batches, from the training set scale well with data size but can converge slowly. See \citet{Goodfellow-et-al-2016}, \citet{Hinton:SGD} or \citet{Ruder:Overview} for clear introductions to these methods. As the data is drawn from small batches recovering the local curvature may become unstable. Furthermore, finding an accurate step-length becomes problematic and more ad-hoc methods are required to improve convergence. These include but are not limited to Momentum \cite{Momentum:Qian}, Adagrad \cite{adagrad:Duchi}, and Adam \cite{adam:Kingma}.

From these observations it is easy to argue for the inclusion of second-order schemes together with batch sampling to improve convergence when training neural networks. This paper is not the first to make this proposal, and there is a significant literature on this approach. Some examples of recent papers are;  \citet{Asi:BetterStoch}, \citet{Le:OptDeep}, \citet{Martens:Deep}, \citet{Xu:2ndNCML} and  \citet{Zhao:SLBFGS}.

\citet{Le:OptDeep}, show that Limited memory BFGS (L-BFGS) and NLCG can significantly speed up and simplify the training of deep algorithms when compared to conventional stochastic gradient descent. The authors claim that L-BFGS is superior for low-dimensional convolutional models, whereas for high dimensional problems NLCG is most competitive. Amongst the algorithms investigated are Restricted Boltzman Machines, auto-encoders, and Sparse RBMs. The authors also discuss the Map-Reduce framework for enhancing parallel computation when training convolutional neural networks CNNs. For this architecture L-BFGS had the best performance. Results were obtained for training on the standard MNIST dataset. The authors also used this dataset to confirm that the use of L-BFGS did not adversely affect classification accuracy. However, Le et al. do not discuss stochastic sampling within second-order schemes and how choices here might impact performance. To an extent the use of parallel computation might mitigate the need for stochastic sampling but this does not resolve the theoretical challenge posed by large-scale data.

\citet{Martens:Deep} looks at second-order optimisation methods for training deep auto-encoders. He achieves superior results to those using first-order methods with pre-training. Martens is in agreement with the present paper that many of the difficulties experienced when training deep networks stem from the choice of first-order methods. Martens mainly focusses on the conjugate-gradient methodology. The paper describes the importance of using fixed batches when combining mini-batch sampling with CG methods and there is a discussion of termination criterion for CG schemes within his Hessian-free (truncated Gauss-Newton) approach. Martens' work is also noticeable for using a preconditioner within the conjugate gradient algorithm. A diagonal Gauss-Newton matrix is used for this which is summed over all samples within each mini-batch. As was found by \citet{Le:OptDeep}, performance was best using relatively large mini-batches. No doubt this helps reduce the influence of outliers within small batches and improves the local curvature estimate.

\citet{Xu:2ndNCML} also present an empirical study of second-order schemes. In this paper, trust region (TR) and adaptive regularization with cubics (ARC) methods are investigated. The authors show that these methods are competitive with SGD using Momentum but have the additional benefit of being highly robust to hyper-parameter settings. Experiments are performed on the CIFAR-10 dataset using shallow networks. Xu et al. make comparisons of TR and ARC with L-BFGS showing that TR and ARC have improved robustness over L-BFGS with respect to the  choice of initialization method. These authors present six criteria to assess second-order optimization in deep learning; \textit{computational efficiency}, \textit{robustness to hyper-parameters}, \textit{escaping saddle-points}, \textit{generalization performance}, \textit{benefits of sub-sampling}, and   \textit{comparison among second-order methods}.

The paper of \citet{Asi:BetterStoch}, is more theoretical and is aimed at improving the robustness of stochastic optimization methods to hyper-parameter choices. The authors define a concept of stability for stochastic solvers and discuss its importance in light of convex optimization theory and proximal methods \citep[see][]{Boyd:Vanden}. The main concern of these authors is improving the sensitivity (stability) of stochastic optimization schemes with respect to the choice of an initial step-length. They are successful in this regard and show that stochastic schemes offer improved convergence for a larger range of initial step-lengths. The authors also compare results with ADAM showing that ADAM can converge faster to a tolerance level but has a narrower range of step-lengths for which it is convergent. Comparisons were made on CIFAR10 (similar to Fashion MNIST) and the Stanford Dogs dataset. The authors use the same approximate Hessian when training Neural networks as was used as a preconditioner by \citet{Martens:Deep}.  \citet{Asi:BetterStoch} are critical of the number of computer and engineer hours which can be wasted by picking hyper-parameters and repeatedly running cross-validation tests on deep networks which can take weeks to train on a single model. They estimate that the energy expended during training of a recurrent deep network \cite{Collinsetal:RNN} is, `sufficient to drive 4,000 Toyota Camrys the 380 miles between San Francisco and Los Angeles'

The paper of \citet{Zhao:SLBFGS} is a mathematically sophisticated work aimed at comparing stochastic L-BFGS methods to its competitors. The authors prove that a stochastic L-BFGS solver will converge linearly in expectation and show that the number of outer iterations to achieve a tolerance of $\epsilon$ is $O(\log(1/\epsilon))$. An algorithm is also provided for implementing stochastic L-BFGS. The authors provide the results of experiments training logistic regression with Stochastic L-BFGS and these were in agreement with the  $O(\log(1/\epsilon))$ sub-optimality bound. This is a significant improvement on the $O(1/\epsilon^2)$ usually cited for stochastic gradient descent \citep[see][]{Srebro:StochOpt}.

The modest aim of this paper is to make empirical comparisons between gradient descent, L-BFGS and NLCG schemes by experimenting with training on the Fashion MNIST dataset. My focus will be on the interaction between the choice of stochastic sampling method and the performance of the solver. I also include an investigation into how a Hessian can be computed for a  Neural network. Both NLCG and L-BFGS-B solvers are available in scipy.optimize \citep[see][SciPy v1.7.1]{scipy:min} and I shall use these for training small networks. I will not use stochastic implementations of these solvers such as those proposed by \citet{Zhao:SLBFGS} or \citet{Hong:SCGM}. The key hypotheses of this work are, 
\begin{itemize}
\item
Including second-order derivatives through the use of an approximate Hessian or using a solver such as NLCG or BFGS \textit{together with} stochastic sampling of mini-batches improves convergence over mini-batch gradient descent alone.\label{hyp1}
\item
Including second-order derivatives in the manner of \ref{hyp1} helps mitigate difficulties such as `vanishing/exploding gradients' and poor line-search estimation.
\end{itemize}
%There are many challenges and questions here. Perhaps the first issue is convergence. Can convergence be guaranteed for stochastic optimization? Moreover, for a given data size how many iterations and epochs (complete runs through the data) are required for a satisfactory reduction in training error. Further to this, how does this convergence estimate scale with batch size. Is there a batch size at which a stochastic implementation is outperformed by classical Newton-type methods. Furthermore what about at test time. The phenomena of double dip descent suggests there might be advantages to stochastic optimization methods at test time, in that the stochastic means that the solution generalizes better to unseen models. Would a stochastic enabled newton type methods show this same behaviour? In the case where stochastic methods dont converge this early stopping can represent a type of regularization, so it is interesting if this effect can be seen with more rapidly converging  schemes.

\section{Second-order optimization algorithms}\label{sec:optim}
Numerical optimisation \citep[see][]{Nocedal:Wright} has a long history dating back to Newton and Gauss. In general, given a loss function $\mathcal{L}$ expressed as a function of a model $m$, with a gradient $\nabla \mathcal{L}$ then Newton's descent step takes the form,
\begin{equation}
m_{(k+1)}=m_{(k)}- \alpha_{k} H_{(k)}^{-1} \nabla \mathcal{L}(m_{(k)}),
\end{equation}
where $\alpha_k$ is the current step-length and $H$\footnote{Note that when $H$ is the identity matrix this is gradient descent.} is the symmetric positive-definite Hessian matrix,
\begin{equation}
H_{ij}=\frac{\partial^2 \mathcal{L}}{\partial m_i \partial m_j}.
\end{equation}
Two issues are immediately apparent, first for large scale nonlinear problems the calculation of $H$ is not straightforward and efficiently calculating $H^{-1}$ raises serious numerical challenges. Both of these problems are encountered when training weights in deep neural networks. For Fashion MNIST, using a single hidden layer the weight matrices had sizes of 784 x 300 for hidden layer and 300 x 10 for output layer with biases taking sizes $300$ and $10$ leading to a total size (for the gradient) of 238,510 or $1.82$ Mb. The Hessian therefore is 238,510 x 238,510 or $3.31$ Mb. 

Algorithms such as nonlinear conjugate gradients (NLCG) and BFGS avoid solving for the inverse Hessian directly and instead solve the equivalent convex-quadratic minimization problem, 
\begin{equation}
j(m)=\frac{1}{2}m^THm-m^T\nabla \mathcal{L}.\label{my_obj}
\end{equation}   
In outline these algorithms perform the iterative procedure, 
\begin{align}
m'_{(k)}&=\argmin{j(m_{(k)})},\\
m_{(k+1)}&=m_{(k)}+\alpha_{k} m'_{(k)}.
\end{align}

In the case of conjugate gradients the minimization is solved iteratively by  decomposing the model descent into a set of residual and conjugate directions. I don't care to review the full details here, \citet{Shew:CGnopain} is an excellent tutorial on both the mathematics and implementation details.

An alternative to NLCG is BFGS which attempts to solve the same minimum problem but builds up approximations to the Hessian by calculating finite-difference derivatives of the gradient. As the Hessian is built up numerically as opposed to relying on an approximate Hessian this scheme has certain convergence advantages over NLCG which uses an inexact Hessian. L-BFGS is a limited memory implementation of BFGS which avoids storing many past iterations of the approximate Hessian.

It is a matter of custom to illustrate the fast convergence of second-order schemes over first-order using the Rosenbrock function. The Rosenbrock function is defined by,
\begin{equation}
\textrm{Rosenb}(x,y)=(a-x)^2+b(y-x^2)^2,\label{rosen_defn}
\end{equation}
with a unique minimum at $(1,1)$. In Figure \ref{Rosenbrock_iter1} I compare the $(x,y)$ iterations in the 2D plane for  i. gradient descent, ii. Newton's method and iii. gradient decent with a line-search estimation.  Convergence plots of the objective function against iteration are shown in Figure \ref{Rosenbrock_conv1}. The superior (quadratic convergence) of Newton's scheme is clear from Figure \ref{Rosenbrock_conv1} whilst gradient descent has difficulty navigating the curved ravine, in Figure \ref{Rosenbrock_iter1}, especially without a line-search. 

\subsection{Preconditioned second-order schemes}
Whilst the computation of $H$ is often intractable, it is often the case that it can be approximated by the positive-semi definite form,
\begin{equation}
H \approx P^T \widehat{H} P,
\end{equation}
where $\widehat{H}$ is computational tractable, for example being diagonal or block-diagonal.  Now, if we define the constrained objective function,
\begin{equation}
\widehat{\mathcal{L}}(p)=\mathcal{L}(m), \hspace{0,3cm} p=Pm
\end{equation}
then,
\begin{align}
P^T \nabla \widehat{\mathcal{L}}&=\nabla \mathcal{L},\\
P^T \nabla^2 \widehat{\mathcal{L}} P&=\nabla^2 \mathcal{L},
\end{align}
and
\begin{equation}
\widehat{j}(p)=\frac{1}{2}p^T \widehat{H} p-p^T\nabla\widehat{\mathcal{L}}.
\end{equation} 
This leads to an efficient second-order scheme, 
\begin{align}
p'_{(k)}&=\argmin{\widehat{j}(P m_{(k)})},\\
m_{(k+1)}&=m_{(k)}+\alpha_{k}P^{-1} p'_{(k)}.
\end{align}
Given a Hessian $H$ a suitable $P$ and $\widehat{H}$ can be found by (block) Cholesky decomposition or (block) LDL decomposition. However, as this is not always practical, it is sufficient to produce an invertible $P$ and a heuristic $\widehat{H}$ which captures salient features of the true Hessian. To this end, \citet{Asi:BetterStoch} and \citet{Martens:Deep} use the approximate Gauss-Newton Hessian,
\begin{equation}
H_k=\Diag{\left(\sum_{i=1}^k \nabla \mathcal{L}_i \nabla \mathcal{L}_i ^T\right)^{\frac{1}{2}}},
\end{equation}
with the sum is taken over all samples within the batch.
\begin{figure}[!ht]
\centering
\includegraphics[height=6cm, width=8cm]{../src-rosen/Rosenbrock_iterations.png}
\caption{Comparison of iterations for minimizing the Rosenbrock function $(a=1.0, b=50.0)$ without noise. Comparisons are made between gradient descent, `Gradient', gradient descent with a line search estimator, `Steplength estimator', and full Newton scheme, `Newton'.}\label{Rosenbrock_iter1}
\end{figure}

\begin{figure}[!ht]
\centering
\includegraphics[height=6cm, width=8cm]{../src-rosen/Rosenbrock_convergence.png}
\caption{Convergence for minimization of the Rosenbrock function $(a=1.0, b=50.0)$ using, gradient descent, `Gradient', gradient descent with a line search estimator, `Steplength estimator', and full Newton scheme, `Newton'.}\label{Rosenbrock_conv1}
\end{figure}

\section{Method}
Succesfully training a neural network involves a number of components, an appropriate labelled dataset, the correct loss function and neural network architecture, a working solver for training the architecture and a well chosen evaluation metric.  All these components must be working for the system to be successful and errors in one aspect may obscure errors in another. For example, a large dataset which is well represented by its test set may hide a poor line-search implementation. For these reasons it is desirable to incrementally test each component. Testing will be carried out for two different different models; i. Logistic regression and ii. softmax classification with a neural network. Doing this will validate different aspects of the system. Logistic regression is a convex function but is trained by the same methods as non-convex neural networks. Therefore, testing with logistic regression validates that numerical optimization is implemented correctly. It also tests the viability of the stochastic sampling method. Neural networks are non-convex, and for a large dataset require batch sampling to train efficiently. Therefore, they represent a true test of optimization performance. 

\subsection{Data} 
When training the logistic regression classifier I used hand-made dataset consisting of 800 pairs of points in the $(x,y)$ plane each with a label, $0$ or $1$. Validation of the classifier (Tables \ref{table_stochlogreg_testerror} and \ref{table_sfixlogreg_testerror}) was carried out using the validation set which consists of 100 pairs of labelled points in the $(x,y)$ as illustrated in Figure \ref{valid_dataset}.

\begin{figure}[!ht]
\centering
\includegraphics[height=6cm, width=8cm]{../src-logistic/logReg_pred_NLCG.png}
\caption{Validation dataset for Logistic regression with the decision boundary found by using an NLCG solver.}\label{valid_dataset}
\end{figure}

When experimenting with the neural network classifier I used the \citet{FashionMNIST:data}, Fashion MNIST dataset. This dataset contains 60,000 training images and 10,000 testing images of clothing types, (labels: T-shirts, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot, with corresponding index numbered 0 - 9). Each image is 28 $\times$ 28 pixels in size, and is generally represented as a flat vector of 784 numbers. The dataset includes labels for each of the ten examples, a number indicating the actual clothing types (0 - 9) in that image. As the labels are not binary a multinomial loss function must be used.

\subsection{Models}   
This project will test convergence of various solvers for two loss functions, logistic loss and softmax (multinomial loss). In this section I define these loss-functions in terms of their weights (models).

\subsubsection*{Logistic regression}
Given data, $X \in \mathbb{R}^{(m,n)}$ consisting of $m$ examples each with $n$ features, and a set of $m$ binary classes for each example, $y\in \{0,1\}^{(m)}$, then we determine a model, $\theta \in \mathbb{R}^n$, which fits $X$ to $y$ by minimizing the logistic-loss, 
\begin{equation}
\mathcal{L}=-\frac{1}{m}\sum_{i}y^{(i)}\log{(\widehat{y}^{(i)})}+(1-y^{(i)})\log{(1-\widehat{y}^{(i)})}.\label{logistic_func}
\end{equation}
Where,
\begin{equation}
\widehat{y}=h_{\theta}(X)=\frac{1}{1+\exp(-X \theta)}.
\end{equation}
The gradient and Hessian of this function are given by,
\begin{align}
\nabla \mathcal{L}&=\frac{1}{m}X^{T}(h_{\theta}(X)-y),\\
\nabla^2 \mathcal{L}&=\frac{1}{m}X^{T}[h_{\theta}(X)(1-h_{\theta}(X))]X.
\end{align}
The Hessian is positive-semi definite and so $\mathcal{L}$ is convex with a well defined minimum. Hence, whilst this function is non-linear on its parameter $\theta$ an optimizer cannot become stuck in a local minima, though clearly some optimization methods will converge faster than others.

\subsubsection*{Neural Networks}
For good performance on Fashion MNIST softmax regression is unlikely to be sufficient. To increase model complexity, I will use a neural network consisting of $L$ layers with the weights of the layers being defined by a list of matrices, $W$ $(=[W_1,W_2,\ldots,W_L])$, and biases, $b$ $(=[b_1,b_2,\ldots,b_L])$. The forward propagation equation for the $l$-th layer takes the form,
\begin{align}
Z^{[l+1](i)}_j&=W^{[l+1]}_{j,k}X^{[l](i)}_k+b^{[l+1]}_j,  \label{propZ}\\
X^{[l+1](i)}_j&=\sigma^{[l+1]}(Z^{[l+1](i)}_j)\label{propX},
\end{align}
where $\sigma^{[l]}$ denotes the choice of activation function for the $l$-th layer. As mentioned, in Fashion MNIST each label is an integer representing a clothing type 0 - 9 and as such $y$ is a multinomial class (with $K=10$) rather than binary.  Therefore, at  the final layer, $L$ I will use the cross-entropy loss function,
\begin{equation}
\mathcal{L}=-\frac{1}{m}\sum_{i}\sum_{k=1}^{K} Y^{(i)}_k\log{(X^{[L](i)}_k)},\label{celossfunc}
\end{equation}
with
\begin{equation}
X^{[L]}=\sigma^{[L]}{(Z^{[L])})},\label{XL_defn}
\end{equation}
where $\sigma$ is the softmax function, 
\begin{equation}
\sigma{(Z_i)}=\frac{\exp{(Z_i)}}{\sum_i \exp{(Z_i)}},\label{softmax}
\end{equation}
and $Y$ is the one-hot matrix representation of the multinomial classes. The final loss function is calculated using the `forward propagation' algorithm (Algorithm \ref{alg:fwd}).

%If the $l$-th layer has $n_l$ features (with $n_0=n$) then,
%\begin{align}
%\textrm{shape}(X^{[l]})&=[n_l,m]=\textrm{shape}(Z^{[l]}),\\
%\textrm{shape}(W^{[l]})&=[n_l,n_{l-1}],\\
%\textrm{shape}(b^{[l]})&=[n_l,].
%\end{align}
%At the final layer we calculate the logistic-loss function,
%\begin{equation}
%\mathcal{L}=-\frac{1}{n}\sum_{i}y^{(i)}\log{(X^{[L](i)})}+(1-y^{(i)})\log{(1-X^{[L](i)})}.\label{lossfunc}
%\end{equation}
%
\begin{algorithm}
\caption{Forward propagation}\label{alg:fwd}
\begin{algorithmic}
%\Procedure {forwardprop}{\textit{list(array)} $W=[W^1,W^2,\ldots, W^L]$,\textit{list(array)} $b=[b^1,b^2,\ldots, b^L]$,$\textit{array}(m,n)$ $X$, $\textit{array}(m)$ $y$}
	\STATE $X^{(0)} \gets X$
	\FOR {$l \leftarrow 1, L$}
		\STATE $Z^{(l)} \gets W^{(l)}X^{(l-1)}+b^{(l)}$
		\STATE $X^{(l)} \gets \sigma(Z^{(l)} )$
	\ENDFOR
	\STATE $\mathcal{L} \gets -\frac{1}{n} \textrm{sum}{\left[\log(X^{(L)})+(1-y)Z^{(L)}\right]}$
%	\State \Return $\mathcal{L}$
%\EndProcedure
\end{algorithmic}
\end{algorithm}

The gradient of the loss function with respect to the set of weights $W$, $\nabla \mathcal{L}$ $(=[\nabla \mathcal{L}^1_{W},\nabla \mathcal{L}^2_{W},\ldots,\nabla \mathcal{L}^L_{W}])$ are calculated by back-propagation. Starting from the final layer,
\begin{equation}
\left[\nabla \mathcal{L}^{[L]}
_{Z}\right]^{(i)}_k=\frac{1}{n}\left[X^{[L](i)}_k-y^{(i)}_k\right].\label{deriv_loss_final}
\end{equation}
The update equation are,
\begin{align}
[\nabla \mathcal{L}^{[l]}_{W}]_{j,k}&=X_{k}^{[l-1](i)}\left[\nabla \mathcal{L}^{[l]}_{Z}\right]^{(i)}_j,\label{gradW}\\
[\nabla \mathcal{L}^{[l]}_{b}]_j&=\sum_{i}\left[\nabla \mathcal{L}^{[l]}_{Z}\right]^{(i)}_j.\label{gradb}
\end{align}
and the backpropagation equation are,
\begin{align}
\left[ \nabla \mathcal{L}^{[l-1]}_{X}\right]^{(i)}_j&=\left(W^{T}\right)_{j,k}^{[l]}\left[\nabla \mathcal{L}^{[l]}_{Z}
\right]^{(i)}_k,\label{backprop_X}\\
\left[\nabla \mathcal{L}_{Z}^{[l-1]}\right]^{(i)}_j&= \sigma'{(Z_{j}^{[l-1](i)})}\left[\nabla \mathcal{L}_{X}^{[l-1]}\right]^{(i)}_j.\label{backprop_Z}\end{align}
Derivations of these equations are given in appendix \ref{backprop_derive}. These equation lead to the well-known `backward propagation' algorithm (Algorithm \ref{alg:bwd}).
\begin{algorithm}
\caption{Backward propagation}\label{alg:bwd}
\begin{algorithmic}
%\Procedure {backwardprop}{\textit{list(array)} $W=[W^1,W^2,\ldots, W^L]$,\textit{list(array)} $Z=[X^{[L]}, X^{[L-1]},\ldots,X^{[0]}$, $\textrm{array}(m)$ $y$}
	\STATE $\nabla \mathcal{L}^{[L]}_{Z} \gets \frac{1}{n}\left[X^{[L]}-y \right]$
	\FOR{$l \leftarrow L, 1$}
		\STATE $\nabla \mathcal{L}^{[l]}_{W} \gets  \nabla \mathcal{L}^{[l]}_{Z}X^{T[l-1]} $
		\STATE $\nabla \mathcal{L}^{[l]}_{b} \gets \textrm{sum}{(\nabla \mathcal{L}^{[l]}_{Z})}$
		\STATE $\nabla \mathcal{L}^{[l-1]}_{X} \gets W^{(l)T}\nabla \mathcal{L}^{[l]}_{Z}$
		\STATE $\nabla \mathcal{L}_{Z}^{[l-1]}\gets \sigma'{(Z^{[l-1]})} \nabla \mathcal{L}_{X}^{[l-1]}$
	\ENDFOR
	%\STATE \Return $[\nabla \mathcal{L}^{[L]}_{W} , \nabla \mathcal{L}^{[L-1]}_{W}, \ldots,  \nabla \mathcal{L}^{[1]}_{W}]$, $[\nabla \mathcal{L}^{[L]}_{b} , \nabla \mathcal{L}^{[L-1]}_{b}, \ldots,  \nabla \mathcal{L}^{[1]}_{b}]$
%\EndProcedure
\end{algorithmic}
\end{algorithm}
A Hessian can be calculated for each layer by taking second derivatives of equations \ref{deriv_loss_final}-\ref{backprop_Z}. The calculations are cumbersome and so the derivation of these equations, together with an algorithm for computing them, is presented in appendix \ref{Hess_prop}.

\subsection{Stochastic sampling strategy}\label{stoch_strategy}
In section \ref{results} we shall see that the stochastic sampling method has a significant influence on the success of higher-order methods. Given $m$ samples we define a batch $\mathcal{B}$ of size, $m_b$, to be a randomly chosen subset of $\{1,2,\ldots,m\}$ chosen without replacement. With this, $X^\mathcal{B}$, $y^\mathcal{B}$ denote a subsampling of the full data, $X$, and labels, $y$ with elements defined by the indices of the entries in  $\mathcal{B}$. The sampling is repeated a number, $R$, times to produce batches  $\mathcal{B}_1, \ldots, \mathcal{B}_R$. From this we define a stochastic objective function suitable for experimentation with a non-linear solver,
\begin{equation}
\mathcal{L}_{sub}(X,y)=\frac{1}{R}\sum_{r=1}^R \mathcal{L}(X^{\mathcal{B}_r}, y^{\mathcal{B}_r}).\label{stoch_resample}
\end{equation}
It would seem more correct to resample the batch $\mathcal{B}$ at each iteration. However, as mentioned by \citet{Martens:Deep}, doing this breaks the assumptions of L-BFGS or NLCG and causes early termination of the solver. Therefore, each batch is fixed throughout training. It is also unnecessary to have repeated coverings of the input data, so in practice I use, $R=\lceil m/m_b \rceil$.

%\begin{algorithm}
%\caption{Backward propagation}\label{alg:bwd}
%\begin{algorithmic}
%\Procedure {backwardprop}{\textit{list(array)} $W=[W^1,W^2,\ldots, W^L]$,\textit{list(array)} $Z=[X^{[L]}, X^{[L-1]},\ldots,X^{[0]}$, $\textrm{array}(m)$ $y$}
%	\State $\nabla \mathcal{L}^{[L]}_{Z} \gets \frac{1}{n}\left[X^{[L]}-y \right]$
%	\For {$l \leftarrow L, 1$}
%		\State $\nabla \mathcal{L}^{[l]}_{W} \gets  \nabla \mathcal{L}^{[l]}_{Z}X^{T[l-1]} $
%		\State $\nabla \mathcal{L}^{[l]}_{b} \gets \textrm{sum}{(\nabla \mathcal{L}^{[l]}_{Z})}$
%		\State $\nabla \mathcal{L}^{[l-1]}_{X} \gets W^{(l)T}\nabla \mathcal{L}^{[l]}_{Z}$
%		\State $\nabla \mathcal{L}_{Z}^{[l-1]}\gets \sigma'{(Z^{[l-1]})} \nabla \mathcal{L}_{X}^{[l-1]}$
%	\EndFor
%	\State \Return $[\nabla \mathcal{L}^{[L]}_{W} , \nabla \mathcal{L}^{[L-1]}_{W}, \ldots,  \nabla \mathcal{L}^{[1]}_{W}]$, $[\nabla \mathcal{L}^{[L]}_{b} , \nabla \mathcal{L}^{[L-1]}_{b}, \ldots,  \nabla \mathcal{L}^{[1]}_{b}]$
%\EndProcedure
%\end{algorithmic}
%\end{algorithm}
\subsection{Evaluation metrics}
I will compare the performance of different solvers based on two metrics: the convergence of the solver during training and the bias-variance diagnostic. The former is the most important for distinguishing between solvers as it tests the rate of convergence. However, the latter is also important as a poorly chosen stochastic algorithm can converge well on a sample of the training set but have poor generalization error on the test set.  It is also good practice to perform a gradient test on any loss function used.  An inaccurate gradient may cause poor solver convergence, and stochastic sampling is likely to exacerbate any unresolved convergence issues. Furthermore, non-linear, non-convex objective functions are known to have saddle-points and local minima which can prevent convergence, or lead to convergence to the wrong minimum. Therefore, any bugs in the implementation risk being confused with a diagnosis of high-bias for a Neural network model.

\subsubsection*{Optimization convergence}\label{metric_conv_curve}
For each solver, the training error will be plotted against the number of iterations during training. This is the key metric for discriminating between solver choices. Better solvers will converge faster. Convergence is defined as the number of iterations, $k_{\epsilon}$, such that,
\begin{equation}
\| f(x_{k})-f(x_{k-1}) \| \le \epsilon.
\end{equation}

\subsubsection*{Bias-Variance Diagnostic}\label{bias_variance}
Dev (or test) accuracy can be defined by a sum over all samples within the dev (or test) set,
\begin{equation}
\textrm{accuracy}(x,y)=\frac{1}{m}\sum_{i=1}^{m}[y^{(i)}=\textrm{predict}(x^{(i)})].
\end{equation}
I will plot training error together with test accuracy for each choice of solver over a range of crucial hyper-parameters. Examples of these hyper-parameters are batch-size, $m_b$, regularization parameter, $\lambda$, and the number of layers in the neural network, $L$. For a purely convex function this test will not be informative since all solvers should converge to the same set of weights albeit at different rates. Thus, if all solvers achieve the same training error then the test error should also be the same for the same dataset. However, this is not necessarily the case when using stochastic sampling as a poorly chosen stochastic algorithm can converge well on a sample of the training set but have poor generalization error on the test set, see Table \ref{table_stochlogreg_testerror} for an example of this.

For a non-convex method the situation is more complicated. Here different solvers may converge to different minima and this impacts the bias-variance tradeoff. For example, if solver $A$ becomes trapped in a local minima whereas $B$ converges then the training error will be less for $A$. However, the solution for $B$ may actually overfit the data whereas $A$ does not. Here, $A$ will be exhibiting high-bias, lower-variance, whereas $B$ will show lower-bias, high-variance. In other words, $B$ is capable of simulating a larger range of models than $A$, therefore is increasing variance. Furthermore, the choice of sampling method will interact with the shape of the loss-function surface obscuring or even reinforcing the problem of saddle-points and local minima. For this reason, it is important to check bias-variance curves when comparing solvers on deep networks.
 
\subsubsection*{Gradient test}\label{grad_test}
Any objective function should be tested to ensure that for randomly chosen $m$ and $\Delta m$ and a suitably small $\epsilon$,
\begin{align}
\frac{1}{2 \epsilon}(\mathcal{L}(m+\epsilon \Delta m)-\mathcal{L}(m-\epsilon \Delta m))=&\Delta m^T \nabla \mathcal{L}\nonumber\\
 &+ O(\epsilon^2). 
\end{align}
 

%\subsection{Error Analysis}\label{err_analysis}
%Given,  data ($X \in \mathbb{R}^{(n,m)},$) and its labels ($y \in \mathbb{R}^m$) a working system must have at least three components,
%\begin{enumerate}
%\item An implementation of a loss function defined over a `model'. This includes,
%\begin{enumerate}
%\item A value of the loss function mapping a set of weights and biases $\mathcal{W}=[(W_1b_1),\ldots, (W_L,b_L)]$ to a (float) value,
%\begin{equation}
%\mathcal{L}: \mathcal{W} \to \mathbb{R}
%\end{equation}
%{\color{red}{[2] - largely implemented previously, though only in outline for deep nets}}, {\color{blue}{[5] - crucial}}.
%\label{objfn_item}
%\item Its gradient
%\begin{equation}
%\nabla \mathcal{L}: \mathcal{W} \to \mathcal{W},
%\end{equation}
%{\color{red}{[2] - largely implemented previously, though only in outline for deep nets}}, {\color{blue}{[5] -  crucial}}.
%
%\item {[Optional]} Its Hessian, or a means of approximating it,
% \begin{equation}
%\nabla^2 \mathcal{L}: \mathcal{W} \to \mathcal{W}\times \mathcal{W}.
%\end{equation}
%{\color{red}{[5] - the riskiest aspect, difficult to compute}}, {\color{blue}{[2] - would be interesting to test but not essential if using L-BFGS}}.\end{enumerate}
%\item A means for stochastically batch sampling the input data and interfacing this with the loss function. {\color{red}{[4] - risky, robust stochastic solvers is still a research question}}, {\color{blue}{[4] - this is necessary for training a large dataset}}.
%\item A classifier which includes,
%\begin{enumerate}
%\item A method for of fitting the data (calculating weights), using a solver (Gradient Descent, Newton's method with line search, NLCG or L-BFGS). {\color{red}{[3] - these functions exists in scipy}}, {\color{blue}{[4] - we dont need to compare all these options, but at least a couple}}.
%\item A method for generating predictions on unseen data. {\color{red}{[1] - already satisfied by \ref{objfn_item}}}, {\color{blue}{[1] - only important for bias-variance analysis}}.
%\end{enumerate}
%\end{enumerate}



\section{Results}\label{results}
\subsection{Logistic regression} 
In Figure \ref{Stoch_Logistic_regression_conv}, I compare the convergence rate between i. NLCG, ii. Newtons' method with line-search, iii. Newtons' method without line-search adaption, iv. L-BFGS during training on batches of the dataset using stochastic sampling. A batch size of 10 samples was chosen and the batches were reshuffled at each iteration. The sampling was repeated `Nbatches' times when calculating the objective function following the method of equation \ref{stoch_resample}. Note that due to reshuffling of the batches the NLCG and BFGS solvers terminated quickly. The corresponding test-error for this sampling method is shown in Table \ref{table_stochlogreg_testerror}
\begin{table}[h!]
\begin{center}
\begin{tabular}{ ccc} %\begin{tabular}{ |ccc|cc|c| } 
\hline
Solver & Train Loss & Test error  \\ 
\hline 
\hline
Newton (LS) & 0.4019 & 0.83\\ 
Newton (noLS) & 0.5613 & 0.81\\ 
L-BFGS & 0.6030 & \textbf{0.51}\\ 
NLCG & 0.5989 & \textbf{0.51}\\ 
\end{tabular}
\end{center}
\caption{Number of iterations and test error for stochastic logistic regression using a batch size of 10, with sampling repeated 50 times and using reshuffling at each iteration. Note that L-BFGS and NLCG have very poor performance on the test-set.}\label{table_stochlogreg_testerror}
\end{table}

In contrast to the reshuffling method, Figure \ref{StochFix_Logistic_regression_conv} shows the convergence rate for i. NLCG, ii. Newtons' method with line-search, iii. Newtons' method without line-search adaption, iv. L-BFGS during training.  A batch size of 10 samples was chosen and the batches were fixed throughout iteration. The sampling was not repeated, however 80 batches were used to cover the full training set.  The corresponding test-error for this sampling method is shown in Table \ref{table_sfixlogreg_testerror}

\begin{table}[h!]
\begin{center}
\begin{tabular}{ cccc} %\begin{tabular}{ |ccc|cc|c| } 
\hline
Solver & Niters & Train Loss & Test error  \\ 
\hline 
\hline
Newton (LS) & 8 & 0.4058 & 0.83\\ 
\textbf{Newton (noLS)} & \textbf{50} & \textbf{0.5617} & \textbf{0.81}\\ 
BFGS & 20 & 0.4058 & 0.83\\ 
NLCG & 31 & 0.4058 & 0.83\\ 
\end{tabular}
\end{center}
\caption{Number of iterations (`Niters')  and test error for logistic regression with a batch size of 10 using various solvers. Newton's method without line search shows higher bias due to early stopping.}\label{table_sfixlogreg_testerror}
\end{table}

\subsection{Fashion MNIST} \label{results_NN}
In Figure \ref{NN_1layer} and \ref{NN_2layer} I compare the convergence rate between i. Gradient descent,  ii. NLCG and  iii. L-BFGS-B during training on batches of Fashion MNIST. The convergence on the dev set is shown and the train-dev accuracy is also plotted. In Figure \ref{NN_1layer}, one hidden layer was used, whereas in Figure \ref{NN_2layer} two hidden layers were used. For these tests, each batch of training data had a size of 1000 x 784, and when using one hidden layer the weights and biases had a sizes of 784 x 300 and 300 respectively. The output layer had a size of 301 x 10. With two hidden layers the weights and biases had sizes of 784 x 300 and 300 (first layer), 300 x100 and 100 (second layer), and 101 x 10 for the output layer. A batch size of 1000 samples was chosen and the batches were fixed throughout each iteration. Whilst the train and dev curves are surprisingly close they are not exactly the same, as is shown in Table \ref{table_fashion_mnist}. Comparisons of the runtime per iteration for each of these solvers is shown in Table \ref{rt_nn}.

\begin{table}[h!]
\begin{center}
\begin{tabular}{ ccc} %\begin{tabular}{ |ccc|cc|c| } 
\hline
Solver & Train Acc & Dev Acc  \\ 
\hline 
\hline
Gradient (1L) & 0.7232 & 0.7195\\ 
NLCG (1L)& 0.8363 & 0.8336\\ 
L-BFGS-B (1L)& 0.8307 & 0.8293\\ 
Gradient (2L)& 0.7029 & 0.7038\\ 
NLCG (2L)& 0.7627 & 0.7650\\ 
L-BFGS-B (2L)& 0.7782 & 0.7845\\ 
\end{tabular}
\end{center}
\caption{Train and Dev accuracy using 100 iterations on Fashion MNIST. $L$ stands for the number of hidden layers.}\label{table_fashion_mnist}
\end{table}

\begin{table}[h!]
\begin{center}
\begin{tabular}{ ccc} %\begin{tabular}{ |ccc|cc|c| } 
\hline
Solver & Runtime (s) & Runtime (s)  \\ 
& &per iteration \\
\hline 
\hline
Gradient (1L)& 411.9 & 8.2\\ 
L-BFGS-B (1L)& 569.3  & 11.3\\ 
NLCG (1L)& 751.1 & 15.0 \\ 
Gradient (2L)& 509.7 & 10.2 \\ 
L-BFGS-B (2L)& 1010.1  & 20.2\\ 
NLCG (2L)& 1215.2 & 24.3 \\ \end{tabular}
\end{center}
\caption{Runtime per iteration for 50 iterations with one (1L) or two (2L) hidden layers and regularization, $\lambda = 0.01$.}\label{rt_nn}
\end{table}

\subsection{Convexity of Neural Networks}\label{app_convex}
In section \ref{results_NN} I avoided testing for a full Newton solver even though the networks were reasonably shallow. Irrespective of any efficiency concerns, I deemed the equations in appendix \ref{Hess_prop} too cumbersome to be implemented quickly and accurately. However, they can still yield important insights into the nature of the loss function surface. For example, we can obtain the following result,
\begin{theorem}
\label{thm:bigtheorem}
If sigmoid activations are used at each hidden layer, and
\begin{equation}
\sgn{\left(Z_j^{[l](i)}\right)}= \sgn{\left(\nabla \mathcal{L}^{[l](i)}_{Z,j} \right)} \hspace{0.2cm} \forall{i,j},\label{convex_condition}
\end{equation}
then the loss function is convex.
\end{theorem}
\begin{proof} 
A result of convex analysis states that an objective function, $f:\mathbb{R}^n \to \mathbb{R}$, is convex if and only if its Hessian is positive semi-definite (PSD), for details see appendix \ref{Convexity}, or page 71 of \citet{Boyd:Vanden},
\begin{equation}
f \hspace{0.1cm} \textrm{is convex} \iff \mathbf{x}^T \nabla^2 f \mathbf{x} \ge 0, \hspace{0.3cm} \forall \mathbf{x} \in \mathbb{R}^n.
\end{equation} 
We note from equation \ref{full_Hdefn} that $H^{[l]}$ is PSD if and only if each $H_Z^{[l]}$ is. That $H_Z^{[l]}$ is PSD follows from induction on the number of layers counting backwards from the output layer $L$.
$H_{Z}^{[L](i)}$ is PSD since,
\begin{align}
\mathbf{y}^T H_{Z}^{[L](i)}\mathbf{y}&=\frac{1}{m} \sum_p  X^{[L](i)}_p y_p^2 -(\sum_q X^{[L](i)}_p y_p)^2,\nonumber\\
&=\frac{1}{m} \sum_p X^{[L](i)}_p(y_p-\sum_q X^{[L](i)}_q y_q)^2
\end{align}
The last expression is $\ge 0$ since $X^{[L](i)}_p \ge 0$ (see equations \ref{XL_defn} and \ref{softmax}).

For the inductive step, assume the result holds at layer $l$. Then, from equations \ref{backprop_HX} and \ref{backprop_HZ}, we find for layer $l-1$,
\begin{align} 
 &\mathbf{y}^T H_{Z}^{[l-1](i)}\mathbf{y}=\nonumber\\
 &\mathbf{y}^T \sigma'{(Z^{[l-1](i)})}\circ W^{T[l]} (H_Z^{[l](i)})W^{[l]}\circ \sigma'{(Z_{k'}^{[l-1](i)})} \mathbf{y}\nonumber\\
 &+\mathbf{y}^{T}\sigma''{(Z^{[l-1](i)})}\circ \left(\nabla \mathcal{L}^{[l-1](i)}_{X}\right) \mathbf{y}.
 \end{align}
The first term in the sum on the right hand side is of the form,
\begin{equation}
 \mathbf{y}'^T  (H_Z^{[l](i)}) \mathbf{y}',
 \end{equation}
and by the inductive hypothesis,
\begin{equation}
 \mathbf{y}'^T  (H_Z^{[l](i)}) \mathbf{y}' \ge 0.
 \end{equation} 
 The second term in the sum will also be positive whenever,
\begin{equation}
\sigma''{(Z_j^{[l-1](i)})} \left(\nabla \mathcal{L}^{[l-1]}_{X}\right)_j^{(i)}\ge 0.
\end{equation}
This occurs whenever,
\begin{equation}
\sgn{\left(Z_j^{[l-1](i)}\right)}= \sgn{\left(\nabla \mathcal{L}^{[l-1](i)}_{X,j} \right)},
\end{equation} 
and from equation \ref{backprop_dL_dZ}, 
\begin{equation}
\sgn{\left(\nabla \mathcal{L}^{[l-1](i)}_{X,j}\right)}= \sgn{\left(\nabla \mathcal{L}^{[l-1](i)}_{Z,j} \right)}.
\end{equation}  
\end{proof}
Theorem \ref{thm:bigtheorem} can provide a means of enforcing convexity during back-propagation. 
\begin{corollary}
If equation \ref{convex_condition} fails to hold at layer $l$ for some $i$, $j$ then we can set,
\begin{equation}
\nabla \mathcal{L}^{[l](i)}_{Z,j} =0.
\end{equation}
\end{corollary}
This says that we can find a convex approximate Hessian suitable as a preconditioner for training the Neural network by filtering the gradient, $\nabla \mathcal{L}_{Z}$, to zero whenever,  
\begin{equation}
\sgn{\left(Z^{[l]}\right)}\neq \sgn{\left(\nabla \mathcal{L}^{[l]}_{Z} \right)}.
\end{equation}
%Unfortunately, this cannot provide a simple recipe to enforce convexity on our Neural Network. To see this, recall we are back-propagating from $l=L$ to $1$, and currently updating at $l-1$. So, if we were to set, 
%\begin{equation}
%\left(\nabla \mathcal{L}^{[l]}_{X}\right)_j^{(i)} = 0,
%\end{equation}
%as a means of enforcing convexity then this would require us to backtrack to the later layer $l$, only to then retrace our steps and this may lead to subsequent convexity failures once we return to $l-1$. However, it does provide a method for detecting when saddle-points occur during back-propagation, and this may have a use when trying to understanding poor convergence.

\section{Discussion}

Figure \ref{Rosenbrock_conv1} shows that second-order solvers have the ability to greatly outperform gradient descent. Figure \ref{StochFix_Logistic_regression_conv} shows that provided fixed batches are used, second-order schemes with line-search outperform Newton methods without line searching. Indeed, the drastic improvement in convergence between using a line-search and using fixed step-size is striking. The explanation is that even with an accurate downhill step, if no adjustment of the step-size is performed by using a line search together with Wolfe conditions \citep[see][]{Nocedal:Wright}, then the step-length can be inappropriate leading to slow convergence. 
 
It is interesting that rapid convergence was restored once resampling of the batches was abandoned and fixed batches were used. To understand this, recall that logistic regression does not contain any hidden layers. Therefore, batch sampling (with fixed batches) followed by regathering the loss function over all batches into the final objective function (equation \ref{stoch_resample}), is equivalent to using all batches in a single objective function. The presence of a hidden layer adds non-linearity and breaks this equivalence.

For the case of training neural networks on Fashion MNIST the situation was less clear cut. It is noticeable that in early iterations the convergence was unstable (most likely the influence of batch sampling) and convergence took a while to `settle down'. However, there was a point when convergence for both L-BFGS and NLCG beat gradient descent and eventually settled down to convergence within 100 iterations, whilst gradient descent still had some way to go. Typically stochastic gradient descent schemes take upwards of $10^4$ iterations. Eventually, an accuracy of $0.83$ was achieved for both second-order solvers on both the train and dev sets. It is also worrying that the statistics for the accuracy of the model on both the train and dev sets were so close. It is possible that this is a feature of the stochastic sampling method, but this matter needs further investigation as this could also be an error. The runtime of NLCG per iteration was almost twice that of gradient descent with L-BFGS-B coming in someway between. This higher cost per iteration might be considered less significant compared to the benefits gained by faster convergence, ie requiring far fewer iterations.

%\subsection{Future Work}
%Three matters require further investigation. The most urgent of these is to explain the very close similarity between the train and dev accuracy and to determine if there is a bug here. 
%
%Secondly, the results of the experiments on Fashion MNIST were somewhat inconclusive. The issue here is that the size of the Fashion MNIST dataset pushes us towards small batches, and so the influence of batch sampling outweighs improvements due purely to using improved solvers. Following on from the case of logistic regression, I now think it would be beneficial to test neural networks on a smaller dataset where batch sampling is not immediately required. When this is done, it should be possible to progressively increase the depth of the network. By removing the influence of batch sampling I can directly test for the onset of pathological convergence problems caused by saddle-points, and exploding-vanishing gradients. This would make for a clearer validation of the performance benefits of second-order schemes. 
%
%Thirdly, my second-order schemes do not make use of a preconditioner and in this sense are behind other empirical studies in the literature such as those of \citet{Martens:Deep}, \citet{Xu:2ndNCML} and \citet{Asi:BetterStoch}. The equations derived in appendix \ref{Hess_prop} provide food for thought on this, and it is possible that simplifications of these equations may provide insights into effective preconditioning schemes for deep networks. Given the very preliminary results obtained in this work I humbly suggest this is worth pursuing. 

\section*{Acnowledgements}
The author would like to thank Andrey Koch for his encouragement and helpful suggestions throughout this project.

\section*{Code availability}
All codes used during the compilation of this paper are available through the GitHub project, \url{https://github.com/bveitch/StochOptDL}

\newpage
\bibliography{Final_Project}  




%\begin{figure}[!ht]
%\centering
%\includegraphics[height=6cm, width=8cm]{../src-rosen/Stochastic_Rosenbrock_iterations.png}
%\caption{Comparison of iterations for minimizing the Rosenbrock function $(a=1.0, b=50.0)$ with Gaussian noise added. Comparisons are made between gradient descent, `Gradient', gradient descent with a line search estimator, `Steplength estimator', and full Newton scheme, `Newton'.}\label{Rosenbrock_iter2}
%\end{figure}
%
%\begin{figure}[!ht]
%\centering
%\includegraphics[height=6cm, width=8cm]{../src-rosen/Stochastic_Rosenbrock_convergence_noise0pt01.png}
%\caption{Convergence for minimization of the Rosenbrock function $(a=1.0, b=50.0)$ with Gaussian noise added ($\sigma=0.01$). `Nbatches' refers to the number of times a batch is sampled, i.e. the number of calls to a noisy Rosenbrock function which were used to estimate an average value (or gradient). Comparisons are made between gradient descent, `Gradient', gradient descent with a line search estimator, `Steplength estimator', and full Newton scheme, `Hessian'.}\label{Rosenbrock_conv2}
%\end{figure}

%\begin{figure}[!ht]
%\centering
%\includegraphics[height=6cm, width=8cm]{../src-logistic/logreg_converge_test.png}
%\caption{Convergence of logistic regression during training for NLCG, Newtons' method with line-search, Newton (LS),  Newtons' method without line-search adaption,  Newton (noLS) and L-BFGS. Note that NLCG and L-BFGS curves lie on top of each other.}\label{Logistic_regression_conv}
%\end{figure}

\begin{figure}[!ht]
\centering
\includegraphics[height=6cm, width=8cm]{../src-logistic/stoch_logreg_converge_test_10.png}
\caption{Convergence of logistic regression during training with mini-batch sampling for NLCG, Newtons' method with line-search, Newton (LS),  Newtons' method without line-search adaption,  Newton (noLS) and L-BFGS. A batch size of 10 samples was chosen and `Nbatches' refers to the number of times the sampling is repeated (with reshuffling) when calculating the objective function defined in equation \ref{stoch_resample}.}\label{Stoch_Logistic_regression_conv}
\end{figure}

\begin{figure}[!ht]
\centering
\includegraphics[height=6cm, width=8cm]{../src-logistic/stoch_logreg_converge_test_batch_size_10.png}
\caption{Convergence of logistic regression during training with mini-batch sampling of 10 samples for NLCG, Newtons' method with line-search, Newton (LS),  Newtons' method without line-search adaption,  Newton (noLS) and L-BFGS. Here the batches were fixed throughout training and $800/10=80$ batches were used.}\label{StochFix_Logistic_regression_conv}
\end{figure}

\begin{figure}[!ht]
\centering
\includegraphics[height=6cm, width=8cm]{../src-mnist/compare_solvers_newtest_100_reg_0pt01_lr_0pt4.pdf}
\caption{Convergence of neural network during training on Fashion MNIST. Training was performed using one hidden layer, for 100 epochs with a batch size of 1000 samples and a regularization parameter, $\lambda=0.01$.}\label{NN_1layer}
\end{figure}

\begin{figure}[!ht]
\centering
\includegraphics[height=6cm, width=8cm]{../src-mnist/compare_solvers_nL_2_nepoch_100_reg_0pt01_lr_0pt4.pdf}
\caption{Convergence of neural network during training on Fashion MNIST. Training was performed using two hidden layers, for 100 epochs with a batch size of 1000 samples and a regularization parameter, $\lambda=0.01$.}\label{NN_2layer}
\end{figure}

\newpage

%
%\begin{figure}[!ht]
%\centering
%\includegraphics[height=6cm, width=8cm]{../src/Rosenbrock_iterations_0pt005_momentum.png}
%\caption{Comparison of iterations for minimizing the Rosenbrock function $(a=1.0, b=50.0)$ with Gaussian noise added. Comparisons are made between gradient descent without momentum, `Gradient, no mom', and with momentum,  `Gradient, mom'.}\label{Rosenbrock_iter3}
%\end{figure}
%
%\begin{figure}[!ht]
%\centering
%\includegraphics[height=6cm, width=8cm]{../src/Rosenbrock_convergence_0pt005_momentum_vs_hess.png}
%\caption{Convergence for minimization of the Rosenbrock function $(a=1.0, b=50.0)$ with Gaussian noise added.  Comparisons are made between gradient descent with momentum, `Momentum' and full Newton scheme without momentum, `Hess, no mom'.}\label{Rosenbrock_conv3}
%\end{figure}



%\subsection{NN forward propagation step}
%Given data consisting of $m$ training examples with $n$ features, we denote the feature vector of the $i$-th training sample by  $X^{(i)}$. The NN will have $L$ layers with the weights of the layers being defined by a list of matrices, $W$ $(=[W_1,W_2,\ldots,W_L])$, and biases, $b$ $(=[b_1,b_2,\ldots,b_L])$. The forward propagation equation for the $l$-th layer takes the form,
%\begin{align}
%Z^{[l+1](i)}_j&=W^{[l+1]}_{j,k}X^{[l](i)}_k+b^{[l+1]}_j,  \label{propZ}\\
%X^{[l+1](i)}_j&=\sigma(Z^{[l+1](i)}_j)\label{propX},
%\end{align}
%where $\sigma$ denotes the sigmoid function, 
%\begin{equation}
%\sigma(z)=\frac{1}{1+e^{-z}}.
%\end{equation}
%Clearly, if the $l$-th layer has $n_l$ features (with $n_0=n$) then,
%\begin{align}
%\textrm{shape}(X^{[l]})&=[n_l,m]=\textrm{shape}(Z^{[l]}),\\
%\textrm{shape}(W^{[l]})&=[n_l,n_{l-1}],\\
%\textrm{shape}(b^{[l]})&=[n_l,].
%\end{align}
%At the final layer we calculate the logistic-loss function,
%\begin{equation}
%\mathcal{L}=-\frac{1}{n}\sum_{i}y^{(i)}\log{(X^{[L](i)})}+(1-y^{(i)})\log{(1-X^{[L](i)})}.\label{lossfunc}
%\end{equation}

%\begin{algorithm}
%\caption{Forward propagation}\label{alg:fwd}
%\begin{algorithmic}
%\Procedure {forwardprop}{\textit{list(array)} $W=[W^1,W^2,\ldots, W^L]$,\textit{list(array)} $b=[b^1,b^2,\ldots, b^L]$,$\textit{array}(m,n)$ $X$, $\textit{array}(m)$ $y$}
%	\State $X^{(0)} \gets X$
%	\For {$l \leftarrow 1, L$}
%		\State $Z^{(l)} \gets W^{(l)}X^{(l-1)}+b^{(l)}$
%		\State $X^{(l)} \gets \sigma(Z^{(l)} )$
%	\EndFor
%	\State $\mathcal{L} \gets -\frac{1}{n} \textrm{sum}{\left(y\log(X^{(L)})+(1-y)\log(1-X^{(L)})\right)}$
%	\State \Return $\mathcal{L}$
%\EndProcedure
%\end{algorithmic}
%\end{algorithm}

%\subsection{NN backward propagation step}
%We find the gradients of the loss function with respect to the set of weights $W$, $\nabla \mathcal{L}$ $(=[\nabla \mathcal{L}^1_{W},\nabla \mathcal{L}^2_{W},\ldots,\nabla \mathcal{L}^L_{W}])$. Starting from the final layer,
%\begin{equation}
%\left[\nabla \mathcal{L}^{[L]}
%_{Z}\right]^{(i)}=\frac{1}{n}\left[X^{[L](i)}-y^{(i)}\right].
%\end{equation}
%The update equation are,
%\begin{align}
%[\nabla \mathcal{L}^{[l]}_{W}]_{j,k}&=X_{k}^{[l-1](i)}\left[\nabla \mathcal{L}^{[l]}_{Z}\right]^{(i)}_j,\label{gradW}\\
%[\nabla \mathcal{L}^{[l]}_{b}]_j&=\sum_{i}\left[\nabla \mathcal{L}^{[l]}_{Z}\right]^{(i)}_j.\label{gradb}
%\end{align}
%and the backpropagation equation are,
%\begin{align}
%\left[ \nabla \mathcal{L}^{[l-1]}_{X}\right]^{(i)}_j&=\left(W^{T}\right)_{j,k}^{[l]}\left[\nabla \mathcal{L}^{[l]}_{Z}
%\right]^{(i)}_k,\label{backprop_X}\\
%\left[\nabla \mathcal{L}_{Z}^{[l-1]}\right]^{(i)}_j&= \sigma'{(Z_{j}^{[l-1](i)})}\left[\nabla \mathcal{L}_{X}^{[l-1]}\right]^{(i)}_j.\label{backprop_Z}\end{align}
%\begin{algorithm}
%\caption{Backward propagation}\label{alg:bwd}
%\begin{algorithmic}
%\Procedure {backwardprop}{\textit{list(array)} $W=[W^1,W^2,\ldots, W^L]$,\textit{list(array)} $Z=[X^{[L]}, X^{[L-1]},\ldots,X^{[0]}$, $\textrm{array}(m)$ $y$}
%	\State $\nabla \mathcal{L}^{[L]}_{Z} \gets \frac{1}{n}\left[X^{[L]}-y \right]$
%	\For {$l \leftarrow L, 1$}
%		\State $\nabla \mathcal{L}^{[l]}_{W} \gets  \nabla \mathcal{L}^{[l]}_{Z}X^{T[l-1]} $
%		\State $\nabla \mathcal{L}^{[l]}_{b} \gets \textrm{sum}{(\nabla \mathcal{L}^{[l]}_{Z})}$
%		\State $\nabla \mathcal{L}^{[l-1]}_{X} \gets W^{(l)T}\nabla \mathcal{L}^{[l]}_{Z}$
%		\State $\nabla \mathcal{L}_{Z}^{[l-1]}\gets \sigma'{(Z^{[l-1]})} \nabla \mathcal{L}_{X}^{[l-1]}$
%	\EndFor
%	\State \Return $[\nabla \mathcal{L}^{[L]}_{W} , \nabla \mathcal{L}^{[L-1]}_{W}, \ldots,  \nabla \mathcal{L}^{[1]}_{W}]$, $[\nabla \mathcal{L}^{[L]}_{b} , \nabla \mathcal{L}^{[L-1]}_{b}, \ldots,  \nabla \mathcal{L}^{[1]}_{b}]$
%\EndProcedure
%\end{algorithmic}
%\end{algorithm}
%\subsection{Hessian propagation}
%We continue the backward propagation equations to calculate the second derivatives. If we restrict attention to finding derivatives only between the hidden units in each layer rather than between layers, the latter being especially cumbersome and unnecessary for our purposes, then we are led to define the second-order derivatives,
%\begin{align}
%(H_W^{[l]})_{jk,j'k'}&=\left(\frac{\partial^2 \mathcal{L}}{\partial W^{[l]}_{j,k} \partial W^{[l]}_{j',k'}}\right)\label{H_W}\\
%(H_b^{[l]})_{j,j'}&=\left(\frac{\partial^2 \mathcal{L}}{\partial b_{j} \partial b_{j'}}\right)^{[l]}\\
%(H_{Wb}^{[l]})_{jk,j'}&=\left(\frac{\partial^2 \mathcal{L}}{\partial W^{[l]}_{j,k} \partial b^{[l]}_{j'}}\right)\\
%(H_Z^{[l]})^{(i)}_{p,q}&=\left(\frac{\partial^2 \mathcal{L}}{\partial Z_{p}^{[l](i)} \partial Z_{q}^{[l](i)}}\right)\\
%(H_X^{[l]})^{(i)}_{p,q}&=\left(\frac{\partial^2 \mathcal{L}}{\partial X_{p}^{[l](i)} \partial X_{q}^{[l](i)}}\right)\label{H_X}
%\end{align}
%Under these definitions a Hessian for each layer can be calculated from\footnote{See equations \ref{H_W_final}, \ref{H_Wb_final}, \ref{H_b_final} in appendix  \ref{Hess_prop}. },
%\begin{align}
%H_W^{[l]}&=(I_{n_l,n_l} \otimes X^{[l-1]})\Diag{(H_Z^{[l](0)},H_Z^{[l](1)},\ldots,H_Z^{[l](m)})}(I_{n_l,n_l} \otimes X^{[l-1]})^T,\label{H_W_calc}\\
%H_{Wb}^{[l]}&=(I_{n_l,n_l} \otimes X^{[l-1]}) [H_Z^{[l](0)},H_Z^{[l](1)},\ldots,H_Z^{[l](m)}]^T\label{H_Wb_calc}\\
%H_{b}^{[l]}&=\sum_i H_Z^{[l](i)}.\label{H_Wb_calc}
%\end{align}
%As in the case of back-propagation the $H_Z^{[l]}$ can be back-propagated through the layers by the rules\footnote{See equation \ref{H_X_final} and \ref{H_Z_final}},
%\begin{align}
%H_X^{[l-1]}&=W^{[l]T} H_Z^{[l]}W^{[l]}.\label{H_X_backprop}\\
%H_Z^{[l-1]}&=\sigma'{(Z^{[l-1]})}\circ H_X^{[l-1]} \circ \sigma'{(Z^{[l-1]})}\color{red}{+\sigma''{(Z^{[l-1]})}\circ \Diag{\left(\nabla \mathcal{L}^{[l]}_{X}\right)}},\label{H_Z_backprop},
%\end{align}
%starting from,
%\begin{equation}
%H_Z^{[L]}=\frac{1}{n}\sigma'{(Z^{[L]})}\label{H_Z_initial}.
%\end{equation}
%A derivation of these statements is given in appendix \ref{Hess_prop}. 
\hrule  
\appendix
\section{Derivation of backward propagation equations}\label{backprop_derive}
Starting from the definition of the cross-entropy loss function \ref{celossfunc}, 
\begin{equation}
\left(\frac{\partial \mathcal{L}}{\partial X}\right)^{[L](i)}_k=-\frac{1}{m}\frac{y^{(i)}_k}{X^{[L](i)}_k},\nonumber
\end{equation}
and so, by the chain-rule the derivatives with respect to $Z$ are,
\begin{equation}
\left(\frac{\partial \mathcal{L}}{\partial Z}\right)^{[L](i)}_k=\left(\frac{\partial \mathcal{L}}{\partial X}\right)^{[L](i)}_r \left(\frac{\partial X^{[L](i)}_r}{\partial Z^{[L](i)}_k}\right),\nonumber
\end{equation}
with,
\begin{equation}
\left(\frac{\partial X^{[L](i)}_r}{\partial Z^{[L](i)}_k}\right)=X_{r}^{[L]i}(\delta_{k,r} - X_{k}^{[L]i}).\nonumber
\end{equation}
Therefore,
\begin{equation}
\left(\frac{\partial \mathcal{L}}{\partial Z}\right)^{[L](i)}_k=\frac{1}{m}\left[X^{[L](i)}_k-y^{(i)}_k\right].\nonumber
\end{equation}
Using the chain-rule for the weights derivatives,
\begin{align}
\left(\frac{\partial \mathcal{L}}{\partial W_{j,k}}\right)^{[l]}&=\sum_{i,p}\left(\frac{\partial \mathcal{L}}{\partial Z_{p}}\right)^{[l](i)}\left(\frac{\partial Z_{p}^{[l](i)}}{\partial W_{j,k}^{[l]}}\right),\label{dL_dW}\\
\left(\frac{\partial \mathcal{L}}{\partial b_{j}}\right)^{[l]}&=\sum_{i,p}\left(\frac{\partial \mathcal{L}}{\partial Z_{p}}\right)^{[l](i)}\left(\frac{\partial Z_{p}^{[l](i)}}{\partial b_{j}^{[l]}}\right).\label{dL_db}
\end{align}
By equation \ref{propZ},
\begin{align}
\frac{\partial Z_{p}^{[l](i)}}{\partial W_{j,k}^{[l]}}&=\delta_{p,j}X_{k}^{[l-1](i)},\label{dZ_dW}\\
\frac{\partial Z_{p}^{[l](i)}}{\partial b_{j}^{[l]}}&=\delta_{p,j},\label{dZ_db}\\
\left(\frac{\partial Z_{p}^{[l]}}{\partial X_{k}^{[l-1]}}\right)^{(i)}&=W_{p,k}^{[l]}.\label{dZ_dX}
\end{align}
Substituting \ref{dZ_dW} into \ref{dL_dW}, and \ref{dZ_db} into \ref{dL_db} leads to,
\begin{align}
\left(\frac{\partial \mathcal{L}}{\partial W_{j,k}}\right)^{[l]}&=\sum_{i}\left(\frac{\partial \mathcal{L}}{\partial Z_{j}}\right)^{[l](i)}X_{k}^{[l-1](i)}\nonumber\\
&=X_{k}^{[l-1],T}\left(\frac{\partial \mathcal{L}}{\partial Z_{j}}\right)^{[l]},\label{wts_update}\\
\left(\frac{\partial \mathcal{L}}{\partial b_{j}}\right)^{[l]}&=\sum_{i}\left(\frac{\partial \mathcal{L}}{\partial Z_{j}}\right)^{[l](i)}\label{bias_update}.
\end{align}
Whilst from equation \ref{dZ_dX},
\begin{align}
\left(\frac{\partial \mathcal{L}}{\partial X_{k}}\right)^{[l-1](i)}&=\sum_{p}\left(\frac{\partial \mathcal{L}}{\partial Z_{p}}\right)^{[l](i)}\left(\frac{\partial Z_{p}^{[l](i)}}{\partial X_{k}^{[l-1](i)}}\right),\nonumber\\
&=\sum_{p} W_{p,k}^{[l]}\left(\frac{\partial \mathcal{L}}{\partial Z_{p}}\right)^{[l](i)},\nonumber\\
&=\left(W^{T}\right)_{k,p}^{[l]}\left(\frac{\partial \mathcal{L}}{\partial Z_{p}}\right)^{[l](i)}\label{backprop_dL_dX}.
\end{align}
Finally, from equation \ref{propX},
\begin{align}
\left(\frac{\partial \mathcal{L}}{\partial Z_{k}}\right)^{[l-1](i)}&=\sum_{p}\left(\frac{\partial \mathcal{L}}{\partial X_{p}}\right)^{[l-1](i)}\left(\frac{\partial X_{p}^{[l-1](i)}}{\partial Z_{k}^{[l-1](i)}}\right),\nonumber\\
&= \sigma'{(Z_{k}^{[l-1](i)})}\left(\frac{\partial \mathcal{L}}{\partial X_{k}}\right)^{[l-1](i)}\label{backprop_dL_dZ}
\end{align}
Equations \ref{backprop_dL_dX} and \ref{backprop_dL_dZ} are the back-propagation equations and used in equations \ref{backprop_X} and \ref{backprop_Z} within the main text. Equations \ref{wts_update} and \ref{bias_update} define the updates for the weights and biases, as used in equations \ref{gradW} and \ref{gradb}.These equations lead to the back-propagation algorithm shown in algorithm \ref{alg:bwd} 

\onecolumn
\section{Second-order derivatives}\label{Hess_prop}
We continue the backward propagation equations to calculate the second derivatives. If we restrict attention to finding derivatives only between the hidden units in each layer rather than between layers, then the Hessian matrix for layer $l$ takes the form,
\begin{equation}
H^{[l]}=\left[
\begin{array}{cc}
H_W^{[l]} & H_{Wb}^{[l]} \\
H_{bW}^{[l]} & H_{b}^{[l]}
\end{array}
\right],
\end{equation}
in which,
\begin{align}
(H_W^{[l]})_{jk,j'k'}&=\frac{\partial^2 \mathcal{L}}{\partial W^{[l]}_{j,k} \partial W^{[l]}_{j',k'}}\label{H_W}\\
(H_b^{[l]})_{j,j'}&=\frac{\partial^2 \mathcal{L}}{\partial b_{j}^{[l]} \partial b_{j'}^{[l]}}\\
(H_{Wb}^{[l]})_{jk,j'}&=\frac{\partial^2 \mathcal{L}}{\partial W^{[l]}_{j,k} \partial b^{[l]}_{j'}}\\
(H_{bW}^{[l]})_{j,j'k'}&=\frac{\partial^2 \mathcal{L}}{\partial b^{[l]}_{j} \partial W^{[l]}_{j',k'} }.
%(H_Z^{[l]})^{(i)}_{p,q}&=\left(\frac{\partial^2 \mathcal{L}}{\partial Z_{p}^{[l](i)} \partial Z_{q}^{[l](i)}}\right)\\
%(H_X^{[l]})^{(i)}_{p,q}&=\left(\frac{\partial^2 \mathcal{L}}{\partial X_{p}^{[l](i)} \partial X_{q}^{[l](i)}}\right)\label{H_X}
\end{align}
Under these definitions a Hessian for each layer can be calculated from the backpropagation equations using the second-order derivatives,
\begin{align}
(H_W^{[l]})_{jk,j'k'}&=(H_Z^{[l]})^{(i)}_{j,j'}X_{k}^{[l-1](i)}X_{k'}^{[l-1](i)},\label{d2L_dW2}\\
(H_b^{[l]})_{j,j'}&=\sum_{i}(H_Z^{[l]})^{(i)}_{j,j'},\label{d2L_db2}\\
(H_{Wb}^{[l]})_{jk,j'}&=\sum_{i} X_{k}^{[l-1](i)}(H_Z^{[l]})^{(i)}_{j,j'},\\
(H_{bW}^{[l]})_{j,j'k'}&=\sum_{i} (H_Z^{[l]})^{(i)}_{j,j'}X_{k'}^{[l-1](i)},%&=X_{k}^{[l-1],T}\left(\frac{\partial^2 \mathcal{L}}{\partial Z_{j}\partial Z_{j'}}\right)^{[l]}\label{d2L_dWdb},
\end{align}
where,
\begin{equation}
(H_Z^{[l]})^{(i)}_{j,j'}=\frac{\partial^2 \mathcal{L}}{\partial Z_{j}^{[l](i)} \partial Z_{j'}^{[l](i)}}\label{H_Z_ijj}.
\end{equation}
Equations \ref{d2L_dW2} to \ref{H_Z_ijj} give rise to an expression for the Hessian for each layer in the matrix form,
\begin{align}
H_W^{[l]}&=(I_{n_l,n_l} \otimes X^{[l-1]})\widehat{H}_Z^{[l]}(I_{n_l,n_l} \otimes X^{[l-1]})^T,\label{H_W_calc}\\
H_{Wb}^{[l]}&=(I_{n_l,n_l} \otimes X^{[l-1]}) \widehat{H}_Z^{[l]}(I_{n_l,n_l} \otimes \mathbf{1}_{n_l})^T,\label{H_Wb_calc}\\
H_{bW}^{[l]}&=(I_{n_l,n_l} \otimes \mathbf{1}_{n_l})\widehat{H}_Z^{[l]}(I_{n_l,n_l} \otimes X^{[l-1]})^T,\label{H_bW_calc}\\
H_{b}^{[l]}&=(I_{n_l,n_l} \otimes \mathbf{1}_{n_l})\widehat{H}_Z^{[l]}(I_{n_l,n_l} \otimes \mathbf{1}_{n_l})^T,\label{H_b_calc}
\end{align}
where,
\begin{equation}
\widehat{H}_Z^{[l]}=\Diag{(H_Z^{[l](0)},H_Z^{[l](1)},\ldots,H_Z^{[l](m)})},
\end{equation}
and $A \otimes B$ denotes the tensor product,
\begin{equation}
A \otimes B=\left[\begin{array}{cccc}
A b_{11} & A b_{12} & \ldots  &A b_{1n}\\
A b_{21} & A b_{22} & \ldots &A b_{2n}\\
\vdots & \vdots & \ddots &\vdots\\
A b_{m1} & A b_{m2} & \ldots &A b_{mn}\\
\end{array}
\right].
\end{equation}
Consequently, the full Hessian matrix can be written in the form,
\begin{equation}
H^{[l]}=\widehat{X}^{[l-1]} \widehat{H}_Z^{[l]}\widehat{X}^{[l-1]T},\label{full_Hdefn}
\end{equation}
with,
\begin{equation}
\widehat{X}^{[l-1]}= \left[
\begin{array}{cc}
(I_{n_l,n_l} \otimes X^{[l-1]}) & 0 \\
0 & (I_{n_l,n_l} \otimes \mathbf{1}_{n_l})
\end{array}
\right] .
\end{equation}
It remains to determine the back-propagation equations for the inner-matrices, $H_Z^{[l](i)}$, we find that in moving from layer $l$ to $l-1$ we have
\begin{align}
(H_X^{[l-1](i)})_{k,k'}&=\left(W^{T}\right)_{k,p}^{[l]} (H_Z^{[l](i)})_{p,q}W_{q,k'}^{[l]}.\label{backprop_HX}\\
(H_Z^{[l-1](i)})_{k,k'}&=\sigma'{(Z_{k}^{[l-1](i)})}(H_X^{[l-1](i)})_{k,k'} \sigma'{(Z_{k'}^{[l-1](i)})}+\sigma''{(Z_{k}^{[l-1](i)})}\delta_{k,k'}\left(\frac{\partial \mathcal{L}}{\partial X_{k}}\right)^{[l-1](i)}\label{backprop_HZ}.
%(H_Z^{[l-1](i)})_{k,k'}&=\sigma'{(Z_{k}^{[l-1](i)})}\left(W^{T}\right)_{k,p}^{[l]} (H_Z^{[l](i)})_{p,q}W_{q,k'}^{[l]}\sigma'{(Z_{k'}^{[l-1](i)})}+\sigma''{(Z_{k}^{[l-1](i)})}\delta_{k,k'}\left(W^{T}\right)_{k,p}^{[l]}\left[\nabla \mathcal{L}^{[l]}_{Z}\right]^{(i)}_p.
\end{align}
Where the propagation starts from layer $L$ where,
\begin{equation}
H_{Z,p,q}^{[L](i)}=\frac{1}{m} X^{[L](i)}_p\left(\delta_{p,q}-X^{[L](i)}_q \right)\label{H_Z_initial}.
\end{equation}
Equations \ref{full_Hdefn}, \ref{backprop_HX} and \ref{backprop_HZ} define an algorithm for back-propagating the Hessian from layer $L$ to $1$.
\section{Convexity and positive semi-definiteness}\label{Convexity}
For completeness we include a proof of the theorem,
\begin{theorem}
\label{thm:convexity}
A smooth objective function $f$, $f:\mathbf{R}^n \to \mathbf{R}$ is convex if and only if its Hessian, $\nabla^2 f $, is positive semi-definite.
\end{theorem}
First, we will require a preparatory lemma,
\begin{lemma}
\label{prep_lemma}
Let $f : \mathbf{R}^n \to \mathbf{R}$ and let $\mathcal{D} $ be a convex subset of $\mathbf{R}^n$. Then, $f$ is convex if and only if for
any $\mathbf{x}, \mathbf{y} \in \mathcal{D}$ we have,
\begin{equation}
f(\mathbf{y}) \ge f(\mathbf{x}) + \nabla f(\mathbf{x})^T (\mathbf{y} - \mathbf{x}).
\end{equation}
\end{lemma}
\begin{proof} 
\begin{trivlist}
\item{(only if) Let $\mathbf{z}=\lambda \mathbf{y} +(1-\lambda)\mathbf{x}$, for $\lambda \in [0,1]$, and assuming $f$ is convex, 
\begin{equation}
f(\mathbf{z})\le \lambda f(\mathbf{y}) +(1-\lambda)f(\mathbf{x}),
\end{equation}
and
\begin{equation}
f(\mathbf{z})-f(\mathbf{x})\le \lambda (f(\mathbf{y}) -f(\mathbf{x})).
\end{equation}
On the other hand,
\begin{align}
\nabla f(\mathbf{x})^T(\mathbf{y}-\mathbf{x})&=\lim_{h \to 0} \frac{f(\mathbf{x}+\lambda(\mathbf{y}-\mathbf{x}))-f(\mathbf{x})}{\lambda}=\lim_{h \to 0} \frac{f(\mathbf{z})-f(\mathbf{x})}{\lambda}\nonumber\\
& \le f(\mathbf{y}) -f(\mathbf{x}).
\end{align}
}
\item{(if)} Now we assume that,
\begin{equation}
f(\mathbf{y}) \ge f(x) + \nabla f(\mathbf{x})^T (\mathbf{y} - \mathbf{x}), \hspace{0.3cm} \forall \mathbf{x}, \mathbf{y} \in \mathcal{D}, \label{grad_bound}
\end{equation}
to show $f$ is convex, let $\mathbf{z}=\lambda \mathbf{y}+(1-\lambda)\mathbf{x}$, for $\lambda \in [0,1]$. Since $\mathcal{D}$ is convex, $z \in \mathcal{D}$, and using \ref{grad_bound} we have,
\begin{align}
f(\mathbf{y}) & \ge f(\mathbf{z}) + \nabla f(\mathbf{z})^T (\mathbf{y} - \mathbf{z}), \label{y_bound_z}\\
f(\mathbf{x}) & \ge f(\mathbf{z}) + \nabla f(\mathbf{z})^T (\mathbf{x} - \mathbf{z}). \label{x_bound_z}
\end{align}
$\lambda \times$ (\ref{y_bound_z}) $+ (1-\lambda)\times $(\ref{x_bound_z})$\implies$
\begin{equation}
\lambda f(\mathbf{y})+(1-\lambda)f(\mathbf{x}) \ge f(\mathbf{z}).
\end{equation}
\end{trivlist}
\end{proof}
Now we are in a position to prove Theorem \ref{thm:convexity},
\begin{proof}
\begin{trivlist}
\item{(only if) By the multivariate Taylor's theorem, $\exists z \in [\mathbf{x},\mathbf{y}]$,
\begin{equation}
f(\mathbf{y})=f(\mathbf{x})+ \nabla f^{T} (\mathbf{y}-\mathbf{x})+\frac{1}{2}(\mathbf{y}-\mathbf{x}) \nabla^2 f(\mathbf{z}) (\mathbf{y}-\mathbf{x}),
\end{equation}
and by positive-definiteness of $\nabla^2 f$,
\begin{equation}
f(\mathbf{y})\ge f(\mathbf{x})+ \nabla f^{T} (\mathbf{y}-\mathbf{x}).
\end{equation}
So, Lemma \ref{prep_lemma} $\implies$ $f$ is convex. 
}
\item{(if) Given $\mathbf{x} \in \mathcal{D}$ and an arbitrary direction $\mathbf{v} \in \mathbf{R}^n$, we can define the 1D function,
\begin{equation}
g(t)=f(\mathbf{x}+t\mathbf{v}),
\end{equation}
Clearly, $g$ is smooth and convex for all $t$ in some interval $[0,\epsilon]$ since $f$ is smooth and convex in $\mathcal{D}$. Further,
\begin{equation}
g''(t)=\mathbf{v}^T\nabla^2 f(\mathbf{x}+t\mathbf{v})\mathbf{v}.
\end{equation}
Since $g$ is a 1D convex function, $g''\ge 0$ for $t \in [0,\epsilon]$ and hence $\nabla^2 f$ is positive semi-definite.
}
\end{trivlist}
\end{proof}
%and the Hessian terms are,
%\begin{align}
%(H_W^{[l]})&=(X^{[l-1]}\otimes I)\cdot \Diag{(I \otimes H_Z^{[l]})}\cdot (X^{[l-1]}\otimes I)^{T},\\
%(H_{Wb}^{[l]})&=(I \otimes H_Z^{[l]})\cdot (X^{[l-1]}\otimes I)^{T},\\
%(H_b^{[l]})&=\sum_{i}(H_Z^{[l]})^{(i)}.
%\end{align}
%\begin{algorithm}
%\caption{Backward propagation with Hessian}\label{alg:backhess}
%\begin{algorithmic}
%\Procedure {backwardhessprop}{\textit{list(array)} $W=[W^1,W^2,\ldots, W^L]$,\textit{list(array)} $Z=[X^{[L]}, X^{[L-1]},\ldots,X^{[0]}$, $\textrm{array}(m)$ $y$}
%	\State $\nabla \mathcal{L}^{[L]}_{Z} \gets \frac{1}{n}\left[X^{[L]}-y \right]$
%	\State $H^{[L]}_{Z} \gets \frac{1}{n}(Z^{L})(1-Z^{L})$	
%	\For {$l \leftarrow L, 1$}
%		\State $\nabla \mathcal{L}^{[l]}_{W} \gets  \nabla \mathcal{L}^{[l]}_{Z}X^{T[l-1]} $
%		\State $\nabla \mathcal{L}^{[l]}_{b} \gets \textrm{sum}{(\nabla \mathcal{L}^{[l]}_{Z})}$
%		\State $[H^{[l]}_W,H^{[l]}_{Wb},H^{[l]}_b] \gets \textrm{Hess}(H^{[l]}_Z, X^{[l-1]})$
%		\State \Comment $\textrm{shape}(H^{[l]}_W)=[n_{l}n_{l-1},n_{l}n_{l-1}]$
%		\State \Comment $\textrm{shape}(H^{[l]}_{Wb})=[n_{l}n_{l-1},n_{l}]$
%		\State \Comment $\textrm{shape}(H^{[l]}_{b})=[n_{l},n_{l}]$			
%		\State $H^{[l]} \gets [[H^{[l]}_W,H^{[l]}_{Wb}],[H^{[l]T}_{Wb},H^{[l]}_b]]$	
%		\State $\nabla \mathcal{L}^{[l-1]}_{X} \gets W^{(l)T}\nabla \mathcal{L}^{[l]}_{Z}$
%		\State $H^{[l-1]}_{X} \gets W^{(l)T} H_Z^{[l]}W^{[l]}$			
%		\State \Comment As shapes: $[n_{l-1},m,n_{l-1}] \gets [n_{l-1},n_l] [n_{l},m,n_{l}] [n_l, n_{l-1}]$	
%		\State $\nabla \mathcal{L}_{Z}^{[l-1]}\gets \sigma'{(Z^{[l-1]})} \nabla \mathcal{L}_{X}^{[l-1]}$
%		\State $H^{[l-1]}_{Z} \gets \sigma'{(Z^{[l-1]})} H_X^{[l-1]}  \sigma'{(Z^{[l-1]})}+\sigma''{(Z^{[l-1]})} \Diag{\left(\nabla \mathcal{L}^{[l]}_{X}\right)}$
%		\State \Comment As shapes: $[n_{l-1},m,n_{l-1}] \gets [n_{l-1},m,n_{l-1}] + [n_{l-1},m,n_{l-1}] $	
%	\EndFor
%	\State \Return $[[\nabla \mathcal{L}^{[L]}_{W} ,\mathcal{L}^{[L]}_{b} ],[ \nabla \mathcal{L}^{[L-1]}_{W},  \nabla \mathcal{L}^{[L-1]}_{b}]\ldots, [ \nabla \mathcal{L}^{[1]}_{W}, \nabla \mathcal{L}^{[1]}_{b}]]$, $[H^{[L]} ,H^{[L-1]}, \ldots, H^{[1]}]$
%\EndProcedure
%\Statex
%\Procedure {Hess}{$H^{[l]}_Z$, $X^{[l-1]}$}
%	\State $[n_{l},m,n_{l}] \gets \textrm{shape}(H^{[l]}_Z)$, $[n_{l-1},m] \gets \textrm{shape}(X^{[l-1]})$
%	\State $Y \gets np.kron(I(n_{l},n_{l}),X^{[l-1]})$
%	\State $D \gets np.zeros(n_l \times m, n_l \times m)$
%	\State $E \gets np.zeros(n_l \times m, n_l)$	
%	\For {$i \leftarrow 0, m$}
%		\State $D[i n_l:(i+1) n_l, i n_l:(i+1) n_l] \gets H^{[l]}_Z[:,i,:]$
%		\State $E[i n_l:(i+1) n_l, n_l] \gets H^{[l]}_Z[:,i,:]$	
%	\EndFor
%	\State $H^{[l]}_W \gets Y \times D \times Y^T$
%	\State $H^{[l]}_{Wb} \gets Y \times E$
%	\State $H^{[l]}_{b} \gets \textrm{sum}(H^{[l]}_Z[:,i,:],i)$
%	\State \Return $[H^{[l]}_W, H^{[l]}_{Wb}, H^{[l]}_{b} ]$	
%\EndProcedure
%\end{algorithmic}
%\end{algorithm}
\twocolumn



\end{document}


